import streamlit as st
import requests
import pandas as pd
import openai
import xml.etree.ElementTree as ET
import re
import io
import xml.etree.ElementTree as ET
from collections import Counter
from bs4 import BeautifulSoup
from openai import OpenAI
from requests.adapters import HTTPAdapter, Retry
from concurrent.futures import ThreadPoolExecutor

# â”€â”€ í•œ ë²ˆë§Œ ìƒì„±: êµ­ì¤‘APIìš© ì„¸ì…˜ & ì¬ì‹œë„ ì„¤ì •
_nlk_session = requests.Session()
_nlk_session.mount(
    "https://",
    HTTPAdapter(
        max_retries=Retry(
            total=1,                # ì¬ì‹œë„ 1íšŒ
            backoff_factor=0.5,     # 0.5ì´ˆ ê°„ê²©
            status_forcelist=[429,500,502,503,504]
        )
    )
)

# âœ… API í‚¤ (secrets.tomlì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
openai_key = st.secrets["api_keys"]["openai_key"]
aladin_key = st.secrets["api_keys"]["aladin_key"]
nlk_key = st.secrets["api_keys"]["nlk_key"]

gpt_client = OpenAI(api_key=openai_key)

# ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ (konlpy ì—†ì´)
def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]

# ğŸ“š ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_category_keywords(category_str):
    keywords = set()
    lines = category_str.strip().splitlines()
    for line in lines:
        parts = [x.strip() for x in line.split('>') if x.strip()]
        if parts:
            keywords.add(parts[-1])
    return list(keywords)

# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ
# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ (OpenAI 1.6.0+ ë°©ì‹ìœ¼ë¡œ ë¦¬íŒ©í† ë§)
def recommend_kdc(title, author, api_key):
    try:
        # ğŸ”‘ ë¹„ë°€ì˜ ì—´ì‡ ë¡œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ê¹¨ì›ë‹ˆë‹¤
        client = OpenAI(api_key=api_key)

        # ğŸ“œ ì£¼ë¬¸ë¬¸ì„ ì¤€ë¹„í•˜ê³ 
        prompt = (
            f"ë„ì„œ ì œëª©: {title}\n"
            f"ì €ì: {author}\n"
            "ì´ ì±…ì˜ ì£¼ì œë¥¼ ê³ ë ¤í•˜ì—¬ í•œêµ­ì‹­ì§„ë¶„ë¥˜(KDC) ë²ˆí˜¸ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\n"
            "ì •í™•í•œ ìˆ«ìë§Œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì‘ë‹µí•´ ì£¼ì„¸ìš”:\n"
            "KDC: 813.7"
        )

        # ğŸ§  GPTì˜ ì§€í˜œë¥¼ ì†Œí™˜
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )

        # âœ‚ï¸ â€œKDC:â€ ë’¤ì˜ ìˆ«ìë§Œ êº¼ë‚´ì„œ ëŒë ¤ë“œë¦½ë‹ˆë‹¤
        for line in response.choices[0].message.content.splitlines():
            if "KDC:" in line:
                return line.split("KDC:")[1].strip()

    except Exception as e:
        st.warning(f"ğŸ§  GPT ì˜¤ë¥˜: {e}")

    # ğŸ›¡ï¸ ë§Œì•½ ì‹¤íŒ¨í•˜ë©´ ë””í´íŠ¸ â€œ000â€
    return "000"


# ğŸ“¡ ë¶€ê°€ê¸°í˜¸ ì¶”ì¶œ (êµ­ë¦½ì¤‘ì•™ë„ì„œê´€)
@st.cache_data(ttl=24*3600)
def fetch_additional_code_from_nlk(isbn: str) -> str:
    url = (
        f"https://www.nl.go.kr/seoji/SearchApi.do?"
        f"cert_key={nlk_key}&result_style=xml"
        f"&page_no=1&page_size=1&isbn={isbn}"
    )
    try:
        res = _nlk_session.get(url, timeout=3)  # 3ì´ˆë§Œ ê¸°ë‹¤ë¦¬ê³ 
        res.raise_for_status()
        root = ET.fromstring(res.text)
        doc  = root.find('.//docs/e')
        return (doc.findtext('EA_ADD_CODE') or "").strip() if doc is not None else ""
    except Exception:
        st.warning("âš ï¸ êµ­ì¤‘API ì§€ì—°, ë¶€ê°€ê¸°í˜¸ëŠ” ìƒëµí•©ë‹ˆë‹¤.")
        return ""


# ğŸ”¤ ì–¸ì–´ ê°ì§€ ë° 041, 546 ìƒì„±
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'

def generate_546_from_041_kormarc(marc_041: str) -> str:
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{a_lang}ë¡œ ì”€, ì›ì €ëŠ” {h_lang}ì„"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

def crawl_aladin_original_and_price(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        price = soup.select_one("span.price2")
        return {
            "original_title": original.text.strip() if original else "",
            "price": price.text.strip().replace("ì •ê°€ : ", "").replace("ì›", "").replace(",", "").strip() if price else ""
        }
    except:
        return {}

# ğŸ“„ 653 í•„ë“œ í‚¤ì›Œë“œ ìƒì„±
# â‘¡ ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° í˜¸ì¶œ í•¨ìˆ˜
def fetch_aladin_metadata(isbn):
    url = (
        "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        f"?ttbkey={aladin_key}"
        "&ItemIdType=ISBN"
        f"&ItemId={isbn}"
        "&output=js"
        "&Version=20131101"
        "&OptResult=Toc" 
    )
    data = requests.get(url).json()
    item = data["item"][0]
    return {
        "category": item.get("categoryName", ""),
        "title": item.get("title", ""),
        "description": item.get("description", ""),
        "toc": item.get("toc", ""),
    }


# â‘¢ GPT-4 ê¸°ë°˜ 653 ìƒì„± í•¨ìˆ˜
def generate_653_with_gpt(category, title, description, toc, max_keywords=7):
    parts = [p.strip() for p in category.split(">") if p.strip()]
    cat_kw = parts[-1] if parts else ""
    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì±…ì˜ ë¶„ë¥˜, ì œëª©, ì„¤ëª…, ëª©ì°¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ "
            "MARC 653 í•„ë“œìš© ì£¼ì œì–´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”."
        )
    }
    user_msg = {
        "role": "user",
        "content": (
            f"ë‹¤ìŒ ì…ë ¥ìœ¼ë¡œ ìµœëŒ€ {max_keywords}ê°œì˜ MARC 653 ì£¼ì œì–´ë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”:\n\n"
            f"- ë¶„ë¥˜: \"{cat_kw}\"\n"
            f"- ì œëª©: \"{title}\"\n"
            f"- ì„¤ëª…: \"{description}\"\n"
            f"- ëª©ì°¨: \"{toc}\"\n\n"
             "â€» â€œì œëª©â€ì— ì‚¬ìš©ëœ ë‹¨ì–´ëŠ” ì œì™¸í•˜ê³ , ìˆœìˆ˜í•˜ê²Œ ë¶„ë¥˜Â·ì„¤ëª…Â·ëª©ì°¨ì—ì„œ ì¶”ì¶œëœ ì£¼ì œì–´ë§Œ ë½‘ì•„ì£¼ì„¸ìš”.\n"
            "ì¶œë ¥ í˜•ì‹: $aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 â€¦"
        )
    }
    try:
        resp = gpt_client.chat.completions.create(
            model="gpt-4",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=150,
        )
        # 1) ì›ë³¸ ì‘ë‹µì„ rawì— ë‹´ìŠµë‹ˆë‹¤
        raw = resp.choices[0].message.content.strip()

        # 2) $a â€¦ ë‹¤ìŒ $a ë˜ëŠ” ëê¹Œì§€ ìº¡ì²˜ (non-greedy)
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]

        # 3) ê° í‚¤ì›Œë“œ ë‚´ë¶€ ê³µë°± ì œê±°
        kws = [kw.replace(" ", "") for kw in kws]

        # 4) ë‹¤ì‹œ "$aí‚¤ì›Œë“œ" í˜•íƒœë¡œ ì¡°ë¦½
        return "".join(f"$a{kw}" for kw in kws)

    except Exception as e:
        st.warning(f"âš ï¸ 653 ì£¼ì œì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    

# â‘£ Streamlit UI
st.title("ğŸ“š ISBN to MARC + 653 ì£¼ì œì–´ ìë™ ìƒì„±")

isbn_input = st.text_input("ISBN ì…ë ¥")
if st.button("ë©”íƒ€ë°ì´í„° ì¡°íšŒ & 653 ìƒì„±"):
    if not isbn_input:
        st.error("ISBNì„ ì…ë ¥í•´ ì£¼ì„¸ìš”.")
    else:
        meta = fetch_aladin_metadata(isbn_input)
        st.subheader("ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„°")
        st.write(meta)

        gpt_653 = generate_653_with_gpt(
            meta["category"],
            meta["title"],
            meta["description"],
            meta["toc"],
        )
        if gpt_653:
            st.subheader("=653")
            st.text_area("MARC 653 ì£¼ì œì–´", gpt_653, height=100)
        else:
            st.error("653 ì£¼ì œì–´ ìƒì„±ì„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")




# ğŸ“š MARC ìƒì„±
@st.cache_data(show_spinner=False)
def fetch_book_data_from_aladin(isbn, reg_mark="", reg_no="", copy_symbol=""):
    import re

    # 1) ì•Œë¼ë”˜(API)ê³¼ êµ­ì¤‘(API) ë¶€ê°€ê¸°í˜¸ë¥¼ ë™ì‹œì— ìš”ì²­í•˜ê¸°
    url = (
        f"https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?"
        f"ttbkey={aladin_key}&itemIdType=ISBN&ItemId={isbn}"
        f"&output=js&Version=20131101"
    )
    with ThreadPoolExecutor(max_workers=2) as ex:
        # 1-1) ì•Œë¼ë”˜ API (5ì´ˆ íƒ€ì„ì•„ì›ƒ)
        future_aladin = ex.submit(lambda: requests.get(url, verify=False, timeout=5))
        # 1-2) êµ­ì¤‘ ë¶€ê°€ê¸°í˜¸ (ìºì‹œ+3ì´ˆ íƒ€ì„ì•„ì›ƒ)
        future_nlk    = ex.submit(fetch_additional_code_from_nlk, isbn)

        # â€” ì•Œë¼ë”˜ ì‘ë‹µ íŒŒì‹±
        try:
            resp = future_aladin.result()
            resp.raise_for_status()
            data = resp.json().get("item", [{}])[0]
        except Exception as e:
            st.error(f"ğŸš¨ ì•Œë¼ë”˜API ì˜¤ë¥˜: {e}")
            return ""

        # â€” êµ­ì¤‘ ë¶€ê°€ê¸°í˜¸ ë°›ê¸° (ì‹¤íŒ¨í•´ë„ ë¹ˆ ë¬¸ìì—´)
        add_code = future_nlk.result()
        st.write("â–¶ [DEBUG] add_code:", repr(add_code))

    # 2) ê¸°ë³¸ í•„ë“œê°’ë“¤
    title       = data.get("title",       "ì œëª©ì—†ìŒ")
    author      = data.get("author",      "ì €ìë¯¸ìƒ")
    publisher   = data.get("publisher",   "ì¶œíŒì‚¬ë¯¸ìƒ")
    pubdate     = data.get("pubDate",     "2025")[:4]
    category    = data.get("categoryName", "")
    description = data.get("description", "")
    toc         = data.get("subInfo", {}).get("toc", "")
    raw_price   = data.get("priceStandard", "")
    price       = str(raw_price)
    st.write("â–¶ priceStandard í™•ì¸:", price)

    # 3) ì–¸ì–´ íƒœê·¸
    lang_a  = detect_language(title)
    lang_h  = detect_language(data.get("title", ""))
    tag_041 = f"=041  \\$a{lang_a}" + (f"$h{lang_h}" if lang_h != "und" else "")
    tag_546 = f"=546  \\$a{generate_546_from_041_kormarc(tag_041)}"

    # 4) 020 í•„ë“œ: ISBN ë’¤ì— :$c{price}ë¥¼ í•­ìƒ ë¶™ì´ê¸°
    tag_020 = f"=020  \\$a{isbn}:$c{price}"
    if add_code:
        tag_020 += f"$g{add_code}"


    # 5) KDCÂ·653
    kdc     = recommend_kdc(title, author, api_key=openai_key)
    # GPT-4ë¡œ 653 ì£¼ì œì–´ ìƒì„± (None ë°˜í™˜ ì‹œ ë¹ˆ ë¬¸ìì—´ ì²˜ë¦¬)
    gpt_653 = generate_653_with_gpt(category, title, description, toc, max_keywords=7)
    tag_653 = f"=653  \\{gpt_653.replace(' ', '')}" if gpt_653 else ""


    # 6) MARC ë¼ì¸ ì´ˆê¸°í™”
    marc_lines = [
        "=007  ta",
        f"=245  00$a{title} /$c{author}",
        f"=260  \\$aì„œìš¸ :$b{publisher},$c{pubdate}.",
    ]

    # 7) 490Â·830 (ì´ì„œëª… + í•­ìƒ ;$v)
    series = data.get("seriesInfo", {})
    name   = series.get("seriesName", "").strip()
    vol    = series.get("volume",    "").strip()
    if name:
        marc_lines.append(f"=490  \\$a{name};$v{vol}")
        marc_lines.append(f"=830  \\$a{name};$v{vol}")

    # 8) ë‚˜ë¨¸ì§€ í•„ë“œ
    marc_lines.append(tag_020)
    marc_lines.append(tag_041)
    marc_lines.append(tag_546)
    if kdc and kdc != "000":
        marc_lines.append(f"=056  \\$a{kdc}$26")
    if tag_653:
        marc_lines.append(tag_653)

    # 9) 950ì€ ë¬´ì¡°ê±´!
    marc_lines.append(f"=950  0\\$b{price}")

    # 10) 049: ì†Œì¥ê¸°í˜¸
    if reg_mark or reg_no or copy_symbol:
        line = f"=049  0\\$I{reg_mark}{reg_no}"
        if copy_symbol:
            line += f"$f{copy_symbol}"
        marc_lines.append(line)

    # 11) ìˆ«ì ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
    marc_lines.sort(key=lambda L: int(re.match(r"=(\d+)", L).group(1)))

    # 12) ìµœì¢… ë¦¬í„´
    return "\n".join(marc_lines)



# ğŸ›ï¸ Streamlit UI
st.title("ğŸ“š ISBN to MARC ë³€í™˜ê¸° (í†µí•©ë²„ì „)")

isbn_list = []
single_isbn = st.text_input("ğŸ”¹ ë‹¨ì¼ ISBN ì…ë ¥", placeholder="ì˜ˆ: 9788936434267")
if single_isbn.strip():
    isbn_list = [[single_isbn.strip(), "", "", ""]]

uploaded_file = st.file_uploader("ğŸ“ CSV ì—…ë¡œë“œ (ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)", type="csv")
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    if {'ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸'}.issubset(df.columns):
        isbn_list = df[['ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸']].dropna(subset=['ISBN']).values.tolist()
    else:
        st.error("âŒ í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸")

if isbn_list:
    st.subheader("ğŸ“„ MARC ì¶œë ¥")
    marc_results = []
    for row in isbn_list:
        isbn, reg_mark, reg_no, copy_symbol = row
        marc = fetch_book_data_from_aladin(isbn, reg_mark, reg_no, copy_symbol)
        if marc:
            st.code(marc, language="text")
            marc_results.append(marc)

    full_text = "\n\n".join(marc_results)
    st.download_button("ğŸ“¦ ëª¨ë“  MARC ë‹¤ìš´ë¡œë“œ", data=full_text, file_name="marc_output.txt", mime="text/plain")

# ğŸ“„ í…œí”Œë¦¿ ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ
example_csv = "ISBN,ë“±ë¡ê¸°í˜¸,ë“±ë¡ë²ˆí˜¸,ë³„ì¹˜ê¸°í˜¸\n9791173473968,JUT,12345,TCH\n"
buffer = io.BytesIO()
buffer.write(example_csv.encode("utf-8-sig"))
buffer.seek(0)
st.download_button("ğŸ“„ ì„œì‹ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=buffer, file_name="isbn_template.csv", mime="text/csv")

# â¬‡ï¸ í•˜ë‹¨ ë§ˆí¬
st.markdown("""
<div style='text-align: center; font-size: 14px; color: gray;'>
ğŸ“š <strong>ë„ì„œ DB ì œê³µ</strong> : <a href='https://www.aladin.co.kr' target='_blank'>ì•Œë¼ë”˜ ì¸í„°ë„·ì„œì (www.aladin.co.kr)</a>
</div>
""", unsafe_allow_html=True)
