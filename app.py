import streamlit as st
import requests
import pandas as pd
import openai
import xml.etree.ElementTree as ET
import re
import io
from collections import Counter
from bs4 import BeautifulSoup
from openai import OpenAI


# âœ… API í‚¤ (secrets.tomlì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
openai_key = st.secrets["api_keys"]["openai_key"]
aladin_key = st.secrets["api_keys"]["aladin_key"]
nlk_key = st.secrets["api_keys"]["nlk_key"]

# ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ (konlpy ì—†ì´)
def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]

# ğŸ“š ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_category_keywords(category_str):
    keywords = set()
    lines = category_str.strip().splitlines()
    for line in lines:
        parts = [x.strip() for x in line.split('>') if x.strip()]
        if parts:
            keywords.add(parts[-1])
    return list(keywords)

# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ
# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ (OpenAI 1.6.0+ ë°©ì‹ìœ¼ë¡œ ë¦¬íŒ©í† ë§)
def recommend_kdc(title, author, api_key):
    try:
        # ğŸ”‘ ë¹„ë°€ì˜ ì—´ì‡ ë¡œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ê¹¨ì›ë‹ˆë‹¤
        client = OpenAI(api_key=api_key)

        # ğŸ“œ ì£¼ë¬¸ë¬¸ì„ ì¤€ë¹„í•˜ê³ 
        prompt = (
            f"ë„ì„œ ì œëª©: {title}\n"
            f"ì €ì: {author}\n"
            "ì´ ì±…ì˜ ì£¼ì œë¥¼ ê³ ë ¤í•˜ì—¬ í•œêµ­ì‹­ì§„ë¶„ë¥˜(KDC) ë²ˆí˜¸ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\n"
            "ì •í™•í•œ ìˆ«ìë§Œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì‘ë‹µí•´ ì£¼ì„¸ìš”:\n"
            "KDC: 813.7"
        )

        # ğŸ§  GPTì˜ ì§€í˜œë¥¼ ì†Œí™˜
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )

        # âœ‚ï¸ â€œKDC:â€ ë’¤ì˜ ìˆ«ìë§Œ êº¼ë‚´ì„œ ëŒë ¤ë“œë¦½ë‹ˆë‹¤
        for line in response.choices[0].message.content.splitlines():
            if "KDC:" in line:
                return line.split("KDC:")[1].strip()

    except Exception as e:
        st.warning(f"ğŸ§  GPT ì˜¤ë¥˜: {e}")

    # ğŸ›¡ï¸ ë§Œì•½ ì‹¤íŒ¨í•˜ë©´ ë””í´íŠ¸ â€œ000â€
    return "000"


# ğŸ“¡ ë¶€ê°€ê¸°í˜¸ ì¶”ì¶œ (êµ­ë¦½ì¤‘ì•™ë„ì„œê´€)
def fetch_additional_code_from_nlk(isbn):
    try:
        url = f"https://www.nl.go.kr/seoji/SearchApi.do?cert_key={nlk_key}&result_style=xml&page_no=1&page_size=10&isbn={isbn}"
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        res.encoding = 'utf-8'
        root = ET.fromstring(res.text)
        doc = root.find('.//docs/e')
        if doc is not None:
            add_code = doc.findtext('EA_ADD_CODE')
            return add_code.strip() if add_code else ""
    except Exception as e:
        st.warning(f"ğŸ“¡ êµ­ì¤‘API ì˜¤ë¥˜: {e}")
    return ""

# ğŸ”¤ ì–¸ì–´ ê°ì§€ ë° 041, 546 ìƒì„±
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'

def generate_546_from_041_kormarc(marc_041: str) -> str:
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{a_lang}ë¡œ ì”€, ì›ì €ëŠ” {h_lang}ì„"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

def crawl_aladin_original_and_price(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        price = soup.select_one("span.price2")
        return {
            "original_title": original.text.strip() if original else "",
            "price": price.text.strip().replace("ì •ê°€ : ", "").replace("ì›", "").replace(",", "").strip() if price else ""
        }
    except:
        return {}

# ğŸ“„ 653 í•„ë“œ í‚¤ì›Œë“œ ìƒì„±
def build_653_field(title, description, toc, raw_category):
    category_keywords = extract_category_keywords(raw_category)
    title_kw = clean_keywords(extract_keywords_from_text(title, 3))
    desc_kw = clean_keywords(extract_keywords_from_text(description, 4))
    toc_kw = clean_keywords(extract_keywords_from_text(toc, 5))
    body_keywords = list(dict.fromkeys(title_kw + desc_kw + toc_kw))[:7]
    combined = category_keywords + body_keywords
    final_keywords = combined[:8]
    if final_keywords:
        return "=653  \\" + "".join([f"$a{kw}" for kw in final_keywords])
    return ""

# ğŸ“š MARC ìƒì„±
@st.cache_data(show_spinner=False)
def fetch_book_data_from_aladin(isbn, reg_mark="", reg_no="", copy_symbol=""):
    try:
        url = (
            f"https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?"
            f"ttbkey={aladin_key}&itemIdType=ISBN&ItemId={isbn}"
            f"&output=js&Version=20131101&optResult=ebookList,reviewList"
        )
        response = requests.get(url, verify=False, timeout=10)
        response.raise_for_status()
        data = response.json().get("item", [{}])[0]
    except Exception as e:
        st.error(f"ğŸš¨ ì•Œë¼ë”˜ API ì˜¤ë¥˜: {e}")
        return ""

    # ê¸°ë³¸ ì •ë³´
    title       = data.get("title",       "ì œëª©ì—†ìŒ")
    author      = data.get("author",      "ì €ìë¯¸ìƒ")
    publisher   = data.get("publisher",   "ì¶œíŒì‚¬ë¯¸ìƒ")
    pubdate     = data.get("pubDate",     "2025")[:4]
    category    = data.get("categoryName","")
    description = data.get("description", "")
    toc         = data.get("subInfo", {}).get("toc", "")

    # í¬ë¡¤ë§ìœ¼ë¡œ ì›ì œÂ·ê°€ê²© ì¶”ì¶œ
    crawl_data     = crawl_aladin_original_and_price(isbn)
    original_title = crawl_data.get("original_title", "")
    price          = crawl_data.get("price", "")

    # ì–¸ì–´ íƒœê·¸
    lang_a = detect_language(title)
    lang_h = detect_language(original_title)
    tag_041 = f"=041  \\$a{lang_a}" + (f"$h{lang_h}" if original_title else "")
    tag_546 = f"=546  \\$a{generate_546_from_041_kormarc(tag_041)}"

    # 020 + ë¶€ê°€ê¸°í˜¸
    tag_020 = f"=020  \\$a{isbn}" + (f":$c{price}" if price else "")
    add_code = fetch_additional_code_from_nlk(isbn)
    if add_code:
        tag_020 += f"$g{add_code}"

    # KDC & 653
    kdc     = recommend_kdc(title, author, api_key=openai_key)
    tag_653 = build_653_field(title, description, toc, category)

    # ê¸°ë³¸ MARC ë¼ì¸
    marc_lines = [
        "=007  ta",
        f"=245  00$a{title} /$c{author}",
        f"=260  \\$aì„œìš¸ :$b{publisher},$c{pubdate}.",
    ]

    # ì´ì„œ(490Â·830)
    series = data.get("seriesInfo", {})  # ì•Œë¼ë”˜ API ì‹¤ì œ í‚¤
    name   = series.get("seriesName", "").strip()
    vol    = series.get("volume",     "").strip()
    if name:
        marc_lines.append(f"=490  \\$a{name};$v{vol}")
        marc_lines.append(f"=830  \\$a{name};$v{vol}")

    # 020Â·056Â·653Â·041Â·546Â·950Â·049 ìˆœì„œëŒ€ë¡œ ì¶”ê°€
    marc_lines.append(tag_020)
    if kdc and kdc != "000":
        marc_lines.append(f"=056  \\$a{kdc}$26")
    if tag_653:
        marc_lines.append(tag_653)
    marc_lines.append(tag_041)
    marc_lines.append(tag_546)
    # 950ì€ ë¬´ì¡°ê±´ ì¶œë ¥ (ê°’ì´ ì—†ìœ¼ë©´ ë¹ˆ \$b)
    marc_lines.append(f"=950  0\\$b{price}")
    # 049: ì†Œì¥ê¸°í˜¸
    if reg_mark or reg_no or copy_symbol:
        line = f"=049  0\\$I{reg_mark}{reg_no}"
        if copy_symbol:
            line += f"$f{copy_symbol}"
        marc_lines.append(line)

    # ìˆ«ì íƒœê·¸ ìˆœì„œëŒ€ë¡œ ê¹”ë”íˆ ì •ë ¬
    import re
    marc_lines.sort(key=lambda L: int(re.match(r"=(\d+)", L).group(1)))

    return "\n".join(marc_lines)



# ğŸ›ï¸ Streamlit UI
st.title("ğŸ“š ISBN to MARC ë³€í™˜ê¸° (í†µí•©ë²„ì „)")

isbn_list = []
single_isbn = st.text_input("ğŸ”¹ ë‹¨ì¼ ISBN ì…ë ¥", placeholder="ì˜ˆ: 9788936434267")
if single_isbn.strip():
    isbn_list = [[single_isbn.strip(), "", "", ""]]

uploaded_file = st.file_uploader("ğŸ“ CSV ì—…ë¡œë“œ (ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)", type="csv")
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    if {'ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸'}.issubset(df.columns):
        isbn_list = df[['ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸']].dropna(subset=['ISBN']).values.tolist()
    else:
        st.error("âŒ í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸")

if isbn_list:
    st.subheader("ğŸ“„ MARC ì¶œë ¥")
    marc_results = []
    for row in isbn_list:
        isbn, reg_mark, reg_no, copy_symbol = row
        marc = fetch_book_data_from_aladin(isbn, reg_mark, reg_no, copy_symbol)
        if marc:
            st.code(marc, language="text")
            marc_results.append(marc)

    full_text = "\n\n".join(marc_results)
    st.download_button("ğŸ“¦ ëª¨ë“  MARC ë‹¤ìš´ë¡œë“œ", data=full_text, file_name="marc_output.txt", mime="text/plain")

# ğŸ“„ í…œí”Œë¦¿ ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ
example_csv = "ISBN,ë“±ë¡ê¸°í˜¸,ë“±ë¡ë²ˆí˜¸,ë³„ì¹˜ê¸°í˜¸\n9791173473968,JUT,12345,TCH\n"
buffer = io.BytesIO()
buffer.write(example_csv.encode("utf-8-sig"))
buffer.seek(0)
st.download_button("ğŸ“„ ì„œì‹ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=buffer, file_name="isbn_template.csv", mime="text/csv")

# â¬‡ï¸ í•˜ë‹¨ ë§ˆí¬
st.markdown("""
<div style='text-align: center; font-size: 14px; color: gray;'>
ğŸ“š <strong>ë„ì„œ DB ì œê³µ</strong> : <a href='https://www.aladin.co.kr' target='_blank'>ì•Œë¼ë”˜ ì¸í„°ë„·ì„œì (www.aladin.co.kr)</a>
</div>
""", unsafe_allow_html=True)
