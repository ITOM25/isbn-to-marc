import streamlit as st
import requests
import pandas as pd
import openai
import xml.etree.ElementTree as ET
import re
import io
from collections import Counter
from bs4 import BeautifulSoup
from openai import OpenAI


# âœ… API í‚¤ (secrets.tomlì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
openai_key = st.secrets["api_keys"]["openai_key"]
aladin_key = st.secrets["api_keys"]["aladin_key"]
nlk_key = st.secrets["api_keys"]["nlk_key"]

# ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ (konlpy ì—†ì´)
def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]

# ğŸ“š ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_category_keywords(category_str):
    keywords = set()
    lines = category_str.strip().splitlines()
    for line in lines:
        parts = [x.strip() for x in line.split('>') if x.strip()]
        if parts:
            keywords.add(parts[-1])
    return list(keywords)

# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ
# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ (OpenAI 1.6.0+ ë°©ì‹ìœ¼ë¡œ ë¦¬íŒ©í† ë§)
def recommend_kdc(title, author, api_key):
    try:
        # ğŸ”‘ ë¹„ë°€ì˜ ì—´ì‡ ë¡œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ê¹¨ì›ë‹ˆë‹¤
        client = OpenAI(api_key=api_key)

        # ğŸ“œ ì£¼ë¬¸ë¬¸ì„ ì¤€ë¹„í•˜ê³ 
        prompt = (
            f"ë„ì„œ ì œëª©: {title}\n"
            f"ì €ì: {author}\n"
            "ì´ ì±…ì˜ ì£¼ì œë¥¼ ê³ ë ¤í•˜ì—¬ í•œêµ­ì‹­ì§„ë¶„ë¥˜(KDC) ë²ˆí˜¸ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\n"
            "ì •í™•í•œ ìˆ«ìë§Œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì‘ë‹µí•´ ì£¼ì„¸ìš”:\n"
            "KDC: 813.7"
        )

        # ğŸ§  GPTì˜ ì§€í˜œë¥¼ ì†Œí™˜
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )

        # âœ‚ï¸ â€œKDC:â€ ë’¤ì˜ ìˆ«ìë§Œ êº¼ë‚´ì„œ ëŒë ¤ë“œë¦½ë‹ˆë‹¤
        for line in response.choices[0].message.content.splitlines():
            if "KDC:" in line:
                return line.split("KDC:")[1].strip()

    except Exception as e:
        st.warning(f"ğŸ§  GPT ì˜¤ë¥˜: {e}")

    # ğŸ›¡ï¸ ë§Œì•½ ì‹¤íŒ¨í•˜ë©´ ë””í´íŠ¸ â€œ000â€
    return "000"


# ğŸ“¡ ë¶€ê°€ê¸°í˜¸ ì¶”ì¶œ (êµ­ë¦½ì¤‘ì•™ë„ì„œê´€)
def fetch_additional_code_from_nlk(isbn):
    try:
        url = f"https://www.nl.go.kr/seoji/SearchApi.do?cert_key={nlk_key}&result_style=xml&page_no=1&page_size=10&isbn={isbn}"
        res = requests.get(url, timeout=10)
        res.raise_for_status()
        res.encoding = 'utf-8'
        root = ET.fromstring(res.text)
        doc = root.find('.//docs/e')
        if doc is not None:
            add_code = doc.findtext('EA_ADD_CODE')
            return add_code.strip() if add_code else ""
    except Exception as e:
        st.warning(f"ğŸ“¡ êµ­ì¤‘API ì˜¤ë¥˜: {e}")
    return ""

# ğŸ”¤ ì–¸ì–´ ê°ì§€ ë° 041, 546 ìƒì„±
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'

def generate_546_from_041_kormarc(marc_041: str) -> str:
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{a_lang}ë¡œ ì”€, ì›ì €ëŠ” {h_lang}ì„"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

def crawl_aladin_original_and_price(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        price = soup.select_one("span.price2")
        return {
            "original_title": original.text.strip() if original else "",
            "price": price.text.strip().replace("ì •ê°€ : ", "").replace("ì›", "").replace(",", "").strip() if price else ""
        }
    except:
        return {}

# ğŸ“„ 653 í•„ë“œ í‚¤ì›Œë“œ ìƒì„±
def build_653_field(title, description, toc, raw_category):
    # 1) ì¹´í…Œê³ ë¦¬ ë§ˆì§€ë§‰ ìš”ì†Œ
    parts   = [p.strip() for p in raw_category.split(">") if p.strip()]
    category = parts[-1] if parts else ""

    # 2) ì œëª©ì—ì„œ ëª…ì‚¬ 2ê°œ, ëª©ì°¨ 5ê°œ, ì„¤ëª… 3ê°œ
    title_kw = clean_keywords(extract_keywords_from_text(title,  top_n=2))
    toc_kw   = clean_keywords(extract_keywords_from_text(toc,    top_n=5))
    desc_kw  = clean_keywords(extract_keywords_from_text(description, top_n=3))

    # 3) ìˆœì„œ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°, ìµœëŒ€ 7ê°œ
    combined = list(dict.fromkeys(title_kw + toc_kw + desc_kw))
    body     = combined[:7]

    # 4) ì¹´í…Œê³ ë¦¬ ì•ì„¸ìš°ê¸°
    final    = ([category] if category else []) + body

    # 5) ì¡°ë¦½
    return "=653  \\" + "".join(f"$a{kw}" for kw in final) if final else ""



# ğŸ“š MARC ìƒì„±
@st.cache_data(show_spinner=False)
def fetch_book_data_from_aladin(isbn, reg_mark="", reg_no="", copy_symbol=""):
    import re

    # 1) API í˜¸ì¶œ
    try:
        url = (
            f"https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?"
            f"ttbkey={aladin_key}&itemIdType=ISBN&ItemId={isbn}"
            f"&output=js&Version=20131101"
        )
        resp = requests.get(url, verify=False, timeout=10)
        resp.raise_for_status()
        data = resp.json().get("item", [{}])[0]
    except Exception as e:
        st.error(f"ğŸš¨ ì•Œë¼ë”˜ API ì˜¤ë¥˜: {e}")
        return ""

    # 2) ê¸°ë³¸ í•„ë“œê°’ë“¤
    title     = data.get("title",       "ì œëª©ì—†ìŒ")
    author    = data.get("author",      "ì €ìë¯¸ìƒ")
    publisher = data.get("publisher",   "ì¶œíŒì‚¬ë¯¸ìƒ")
    pubdate   = data.get("pubDate", "2025")[:4]
    # â€” (ì¶”ê°€) ì¹´í…Œê³ ë¦¬ ì •ë³´ë„ êº¼ë‚´ê¸°
    category  = data.get("categoryName", "")
    # â€” ê°€ê²©: TTB APIì—ì„œ ë°”ë¡œ êº¼ë‚´ë˜, int â†’ str ë³€í™˜
    raw_price = data.get("priceStandard", "")
    price     = str(raw_price)
    # â”€â”€ (ì˜µì…˜) ë””ë²„ê·¸: ê°€ê²©ì´ ì œëŒ€ë¡œ ë“¤ì–´ì˜¤ëŠ”ì§€ í™•ì¸
    st.write("â–¶ priceStandard í™•ì¸:", price)

    # 3) ì–¸ì–´ íƒœê·¸
    lang_a  = detect_language(title)
    lang_h  = detect_language(data.get("title", ""))
    tag_041 = f"=041  \\$a{lang_a}" + (f"$h{lang_h}" if lang_h != "und" else "")
    tag_546 = f"=546  \\$a{generate_546_from_041_kormarc(tag_041)}"

    # 4) 020 í•„ë“œ: ISBN ë’¤ì— :$c{price}ë¥¼ í•­ìƒ ë¶™ì´ê¸°
    tag_020 = f"=020  \\$a{isbn}:$c{price}"
    add_code = fetch_additional_code_from_nlk(isbn)
    if add_code:
        tag_020 += f"$g{add_code}"

    # â€” KDCÂ·653 (ì›ì¹™ëŒ€ë¡œ ì œëª©Â·ëª©ì°¨Â·ì„¤ëª…Â·ì¹´í…Œê³ ë¦¬ë¥¼ ëª¨ë‘ ë„˜ê²¨ ì£¼ê¸°)
    kdc     = recommend_kdc(title, author, api_key=openai_key)
    tag_653 = build_653_field(title, description, toc, category)

    # 6) MARC ë¼ì¸ ì´ˆê¸°í™”
    marc_lines = [
        "=007  ta",
        f"=245  00$a{title} /$c{author}",
        f"=260  \\$aì„œìš¸ :$b{publisher},$c{pubdate}.",
    ]

    # 7) 490Â·830 (ì´ì„œëª… + í•­ìƒ ;$v)
    series = data.get("seriesInfo", {})  
    name   = series.get("seriesName", "").strip()
    vol    = series.get("volume",     "").strip()
    if name:
        marc_lines.append(f"=490  \\$a{name};$v{vol}")
        marc_lines.append(f"=830  \\$a{name};$v{vol}")

    # 8) ë‚˜ë¨¸ì§€ í•„ë“œ (ìˆœì„œëŠ” ì •ë ¬ì—ì„œ ì²˜ë¦¬)
    marc_lines.append(tag_020)                # =020
    marc_lines.append(tag_041)                # =041
    marc_lines.append(tag_546)                # =546
    if kdc and kdc != "000":
        marc_lines.append(f"=056  \\$a{kdc}$26")   # =056
    if tag_653:
        marc_lines.append(tag_653)            # =653

    # 950ì€ ë¬´ì¡°ê±´! (ë¹„ì–´ ìˆì–´ë„ í•„ë“œë§Œ ë‚¨ê¹€)
    marc_lines.append(f"=950  0\\$b{price}")

    # 049: ì†Œì¥ê¸°í˜¸
    if reg_mark or reg_no or copy_symbol:
        line = f"=049  0\\$I{reg_mark}{reg_no}"
        if copy_symbol:
            line += f"$f{copy_symbol}"
        marc_lines.append(line)

    # 9) ìˆ«ì ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬
    marc_lines.sort(key=lambda L: int(re.match(r"=(\d+)", L).group(1)))

    # 10) ìµœì¢… ë¦¬í„´
    return "\n".join(marc_lines)




# ğŸ›ï¸ Streamlit UI
st.title("ğŸ“š ISBN to MARC ë³€í™˜ê¸° (í†µí•©ë²„ì „)")

isbn_list = []
single_isbn = st.text_input("ğŸ”¹ ë‹¨ì¼ ISBN ì…ë ¥", placeholder="ì˜ˆ: 9788936434267")
if single_isbn.strip():
    isbn_list = [[single_isbn.strip(), "", "", ""]]

uploaded_file = st.file_uploader("ğŸ“ CSV ì—…ë¡œë“œ (ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)", type="csv")
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    if {'ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸'}.issubset(df.columns):
        isbn_list = df[['ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸']].dropna(subset=['ISBN']).values.tolist()
    else:
        st.error("âŒ í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸")

if isbn_list:
    st.subheader("ğŸ“„ MARC ì¶œë ¥")
    marc_results = []
    for row in isbn_list:
        isbn, reg_mark, reg_no, copy_symbol = row
        marc = fetch_book_data_from_aladin(isbn, reg_mark, reg_no, copy_symbol)
        if marc:
            st.code(marc, language="text")
            marc_results.append(marc)

    full_text = "\n\n".join(marc_results)
    st.download_button("ğŸ“¦ ëª¨ë“  MARC ë‹¤ìš´ë¡œë“œ", data=full_text, file_name="marc_output.txt", mime="text/plain")

# ğŸ“„ í…œí”Œë¦¿ ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ
example_csv = "ISBN,ë“±ë¡ê¸°í˜¸,ë“±ë¡ë²ˆí˜¸,ë³„ì¹˜ê¸°í˜¸\n9791173473968,JUT,12345,TCH\n"
buffer = io.BytesIO()
buffer.write(example_csv.encode("utf-8-sig"))
buffer.seek(0)
st.download_button("ğŸ“„ ì„œì‹ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=buffer, file_name="isbn_template.csv", mime="text/csv")

# â¬‡ï¸ í•˜ë‹¨ ë§ˆí¬
st.markdown("""
<div style='text-align: center; font-size: 14px; color: gray;'>
ğŸ“š <strong>ë„ì„œ DB ì œê³µ</strong> : <a href='https://www.aladin.co.kr' target='_blank'>ì•Œë¼ë”˜ ì¸í„°ë„·ì„œì (www.aladin.co.kr)</a>
</div>
""", unsafe_allow_html=True)
