import streamlit as st
import requests
import pandas as pd
import io
import datetime
import xml.etree.ElementTree as ET
import re
import unicodedata
from collections import Counter
from bs4 import BeautifulSoup
from openai import OpenAI
from requests.adapters import HTTPAdapter, Retry
from concurrent.futures import ThreadPoolExecutor


# â”€â”€ í•œ ë²ˆë§Œ ìƒì„±: êµ­ì¤‘APIìš© ì„¸ì…˜ & ì¬ì‹œë„ ì„¤ì •
_nlk_session = requests.Session()
_nlk_session.mount(
    "https://",
    HTTPAdapter(
        max_retries=Retry(
            total=1,                # ì¬ì‹œë„ 1íšŒ
            backoff_factor=0.5,     # 0.5ì´ˆ ê°„ê²©
            status_forcelist=[429,500,502,503,504]
        )
    )
)

# âœ… API í‚¤ (secrets.tomlì—ì„œ ë¶ˆëŸ¬ì˜¤ê¸°)
openai_key = st.secrets["api_keys"]["openai_key"]
aladin_key = st.secrets["api_keys"]["aladin_key"]
nlk_key = st.secrets["api_keys"]["nlk_key"]

gpt_client = OpenAI(api_key=openai_key)

# í•œêµ­ ë°œí–‰ì§€ ë¬¸ìì—´ â†’ KORMARC 3ìë¦¬ ì½”ë“œ (í•„ìš” ì‹œ í™•ì¥)
KR_REGION_TO_CODE = {
    "ì„œìš¸": "ulk", "ì„œìš¸íŠ¹ë³„ì‹œ": "ulk",
    "ê²½ê¸°": "ggk", "ê²½ê¸°ë„": "ggk",
    "ë¶€ì‚°": "bnk", "ë¶€ì‚°ê´‘ì—­ì‹œ": "bnk",
    "ëŒ€êµ¬": "tgk", "ëŒ€êµ¬ê´‘ì—­ì‹œ": "tgk",
    "ì¸ì²œ": "ick", "ì¸ì²œê´‘ì—­ì‹œ": "ick",
    "ê´‘ì£¼": "kjk", "ê´‘ì£¼ê´‘ì—­ì‹œ": "kjk",
    "ëŒ€ì „": "tjk", "ëŒ€ì „ê´‘ì—­ì‹œ": "tjk",
    "ìš¸ì‚°": "usk", "ìš¸ì‚°ê´‘ì—­ì‹œ": "usk",
    "ì„¸ì¢…": "sjk", "ì„¸ì¢…íŠ¹ë³„ìì¹˜ì‹œ": "sjk",
    "ê°•ì›": "gak", "ê°•ì›íŠ¹ë³„ìì¹˜ë„": "gak",
    "ì¶©ë¶": "hbk", "ì¶©ì²­ë¶ë„": "hbk",
    "ì¶©ë‚¨": "hck", "ì¶©ì²­ë‚¨ë„": "hck",
    "ì „ë¶": "jbk", "ì „ë¼ë¶ë„": "jbk",
    "ì „ë‚¨": "jnk", "ì „ë¼ë‚¨ë„": "jnk",
    "ê²½ë¶": "gbk", "ê²½ìƒë¶ë„": "gbk",
    "ê²½ë‚¨": "gnk", "ê²½ìƒë‚¨ë„": "gnk",
    "ì œì£¼": "jjk", "ì œì£¼íŠ¹ë³„ìì¹˜ë„": "jjk",
}

# ê¸°ë³¸ê°’: ë°œí–‰êµ­/ì–¸ì–´/ëª©ë¡ì „ê±°
COUNTRY_FIXED = "ulk"   # ë°œí–‰êµ­ ê¸°ë³¸ê°’
LANG_FIXED    = "kor"   # ì–¸ì–´ ê¸°ë³¸ê°’

# 008 ë³¸ë¬¸(40ì) ì¡°ë¦½ê¸° â€” ë‹¨í–‰ë³¸ ê¸°ì¤€(type_of_date ê¸°ë³¸ 's')
def build_008_kormarc_bk(
    date_entered,          # 00-05 YYMMDD
    date1,                 # 07-10 4ìë¦¬(ì˜ˆ: '2025' / '19uu')
    country3,              # 15-17 3ìë¦¬
    lang3,                 # 35-37 3ìë¦¬
    date2="",              # 11-14
    illus4="",             # 18-21 ìµœëŒ€ 4ì(ì˜ˆ: 'a','ad','ado'â€¦)
    has_index="0",         # 31 '0' ì—†ìŒ / '1' ìˆìŒ
    lit_form=" ",          # 33 (pì‹œ/fì†Œì„¤/eìˆ˜í•„/iì„œê°„ë¬¸í•™/mê¸°í–‰Â·ì¼ê¸°Â·ìˆ˜ê¸°)
    bio=" ",               # 34 (a ìì„œì „ / b ì „ê¸°Â·í‰ì „ / d ë¶€ë¶„ì  ì „ê¸°)
    type_of_date="s",      # 06
    modified_record=" ",   # 28
    cataloging_src="a",    # 32  â† ê¸°ë³¸ê°’ 'a'
):
    def pad(s, n, fill=" "):
        s = "" if s is None else str(s)
        return (s[:n] + fill * n)[:n]

    if len(date_entered) != 6 or not date_entered.isdigit():
        raise ValueError("date_enteredëŠ” YYMMDD 6ìë¦¬ ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤.")
    if len(date1) != 4:
        raise ValueError("date1ì€ 4ìë¦¬ì—¬ì•¼ í•©ë‹ˆë‹¤. ì˜ˆ: '2025', '19uu'")

    body = "".join([
        date_entered,               # 00-05
        pad(type_of_date,1),        # 06
        date1,                      # 07-10
        pad(date2,4),               # 11-14
        pad(country3,3),            # 15-17
        pad(illus4,4),              # 18-21
        " " * 4,                    # 22-25 (ì´ìš©ëŒ€ìƒ/ìë£Œí˜•íƒœ/ë‚´ìš©í˜•ì‹) ê³µë°±
        " " * 2,                    # 26-27 ê³µë°±
        pad(modified_record,1),     # 28
        " ",                        # 29 íšŒì˜ê°„í–‰ë¬¼
        " ",                        # 30 ê¸°ë…ë…¼ë¬¸ì§‘
        has_index if has_index in ("0","1") else "0",  # 31 ìƒ‰ì¸
        pad(cataloging_src,1),      # 32 ëª©ë¡ ì „ê±°
        pad(lit_form,1),            # 33 ë¬¸í•™í˜•ì‹
        pad(bio,1),                 # 34 ì „ê¸°
        pad(lang3,3),               # 35-37 ì–¸ì–´
        " " * 2                     # 38-39 (ì •ë¶€ê¸°ê´€ë¶€í˜¸ ë“±) ê³µë°±
    ])
    if len(body) != 40:
        raise AssertionError(f"008 length != 40: {len(body)}")
    return body

# ë°œí–‰ì—°ë„ ì¶”ì¶œ(ì•Œë¼ë”˜ pubDate ìš°ì„ )
def extract_year_from_aladin_pubdate(pubdate_str: str) -> str:
    m = re.search(r"(19|20)\d{2}", pubdate_str or "")
    return m.group(0) if m else "19uu"

# 300 ë°œí–‰ì§€ ë¬¸ìì—´ â†’ country3 ì¶”ë¡ 
def guess_country3_from_place(place_str: str) -> str:
    if not place_str:
        return COUNTRY_FIXED
    for key, code in KR_REGION_TO_CODE.items():
        if key in place_str:
            return code
    # í•œêµ­ ì¼ë°˜ì½”ë“œ("ko ")ëŠ” ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ, ê¸°ë³¸ê°’ìœ¼ë¡œ í†µì¼
    return COUNTRY_FIXED


# ====== ë‹¨ì–´ ê°ì§€ ======
def detect_illus4(text: str) -> str:
    # a: ì‚½í™”/ì¼ëŸ¬ìŠ¤íŠ¸/ê·¸ë¦¼, d: ë„í‘œ/ê·¸ë˜í”„/ì°¨íŠ¸, o: ì‚¬ì§„/í™”ë³´
    keys = []
    if re.search(r"ì‚½í™”|ì‚½ë„|ë„í•´|ì¼ëŸ¬ìŠ¤íŠ¸|ì¼ëŸ¬ìŠ¤íŠ¸ë ˆì´ì…˜|ê·¸ë¦¼|illustration", text, re.I): keys.append("a")
    if re.search(r"ë„í‘œ|í‘œ|ì°¨íŠ¸|ê·¸ë˜í”„|chart|graph", text, re.I):                          keys.append("d")
    if re.search(r"ì‚¬ì§„|í¬í† |í™”ë³´|photo|photograph|ì»¬ëŸ¬ì‚¬ì§„|ì¹¼ë¼ì‚¬ì§„", text, re.I):          keys.append("o")
    out = []
    for k in keys:
        if k not in out:
            out.append(k)
    return "".join(out)[:4]

def detect_index(text: str) -> str:
    return "1" if re.search(r"ìƒ‰ì¸|ì°¾ì•„ë³´ê¸°|ì¸ëª…ìƒ‰ì¸|ì‚¬í•­ìƒ‰ì¸|index", text, re.I) else "0"

def detect_lit_form(title: str, category: str, extra_text: str = "") -> str:
    blob = f"{title} {category} {extra_text}"
    if re.search(r"ì„œê°„ì§‘|í¸ì§€|ì„œê°„ë¬¸|letters?", blob, re.I): return "i"    # ì„œê°„ë¬¸í•™
    if re.search(r"ê¸°í–‰|ì—¬í–‰ê¸°|ì—¬í–‰ ì—ì„¸ì´|ì¼ê¸°|ìˆ˜ê¸°|diary|travel", blob, re.I): return "m"  # ê¸°í–‰/ì¼ê¸°/ìˆ˜ê¸°
    if re.search(r"ì‹œì§‘|ì‚°ë¬¸ì‹œ|poem|poetry", blob, re.I): return "p"        # ì‹œ
    if re.search(r"ì†Œì„¤|ì¥í¸|ì¤‘ë‹¨í¸|novel|fiction", blob, re.I): return "f"  # ì†Œì„¤
    if re.search(r"ì—ì„¸ì´|ìˆ˜í•„|essay", blob, re.I): return "e"               # ìˆ˜í•„
    return " "

def detect_bio(text: str) -> str:
    if re.search(r"ìì„œì „|íšŒê³ ë¡|autobiograph", text, re.I): return "a"
    if re.search(r"ì „ê¸°|í‰ì „|ì¸ë¬¼ í‰ì „|biograph", text, re.I): return "b"
    if re.search(r"ì „ê¸°ì |ìì „ì |íšŒê³ |íšŒìƒ", text): return "d"
    return " "

# ë©”ì¸: ISBN í•˜ë‚˜ë¡œ 008 ìƒì„± (toc/300/041 ì—°ë™ ê°€ëŠ¥)
def build_008_from_isbn(
    isbn: str,
    *,
    aladin_pubdate: str = "",
    aladin_title: str = "",
    aladin_category: str = "",
    aladin_desc: str = "",
    aladin_toc: str = "",            # ëª©ì°¨ê°€ ìˆìœ¼ë©´ ê°ì§€ì— í™œìš©
    source_300_place: str = "",      # 300 ë°œí–‰ì§€ ë¬¸ìì—´(ìˆìœ¼ë©´ country3 ì¶”ì •)
    override_country3: str = None,   # ì™¸ë¶€ ëª¨ë“ˆì´ ì£¼ë©´ ìµœìš°ì„ 
    override_lang3: str = None,      # ì™¸ë¶€ ëª¨ë“ˆì´ ì£¼ë©´ ìµœìš°ì„ (041)
    cataloging_src: str = "a",       # 32 ëª©ë¡ ì „ê±°(ê¸°ë³¸ 'a')
):
    today  = datetime.datetime.now().strftime("%y%m%d")  # YYMMDD
    date1  = extract_year_from_aladin_pubdate(aladin_pubdate)

    # country ìš°ì„ ìˆœìœ„: override > 300ë°œí–‰ì§€ ë§¤í•‘ > ê¸°ë³¸ê°’
    if override_country3:
        country3 = override_country3
    elif source_300_place:
        country3 = guess_country3_from_place(source_300_place)
    else:
        country3 = COUNTRY_FIXED

    # lang ìš°ì„ ìˆœìœ„: override(041) > ê¸°ë³¸ê°’
    lang3 = override_lang3 or LANG_FIXED

    # ë‹¨ì–´ ê°ì§€ìš© í…ìŠ¤íŠ¸: ì œëª© + ì†Œê°œ + ëª©ì°¨
    bigtext = " ".join([aladin_title or "", aladin_desc or "", aladin_toc or ""])
    illus4    = detect_illus4(bigtext)
    has_index = detect_index(bigtext)
    lit_form  = detect_lit_form(aladin_title or "", aladin_category or "", bigtext)
    bio       = detect_bio(bigtext)

    return build_008_kormarc_bk(
        date_entered=today,
        date1=date1,
        country3=country3,
        lang3=lang3,
        illus4=illus4,
        has_index=has_index,
        lit_form=lit_form,
        bio=bio,
        cataloging_src=cataloging_src,
    )
# ========= 008 ìƒì„± ë¸”ë¡ v3 ë =========

# ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ (konlpy ì—†ì´)
def extract_keywords_from_text(text, top_n=7):
    words = re.findall(r'\b[\wê°€-í£]{2,}\b', text)
    filtered = [w for w in words if len(w) > 1]
    freq = Counter(filtered)
    return [kw for kw, _ in freq.most_common(top_n)]

def clean_keywords(words):
    stopwords = {"ì•„ì£¼", "ê°€ì§€", "í•„ìš”í•œ", "ë“±", "ìœ„í•´", "ê²ƒ", "ìˆ˜", "ë”", "ì´ëŸ°", "ìˆë‹¤", "ëœë‹¤", "í•œë‹¤"}
    return [w for w in words if w not in stopwords and len(w) > 1]

# ğŸ“š ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_category_keywords(category_str):
    keywords = set()
    lines = category_str.strip().splitlines()
    for line in lines:
        parts = [x.strip() for x in line.split('>') if x.strip()]
        if parts:
            keywords.add(parts[-1])
    return list(keywords)

# ğŸ”§ GPT ê¸°ë°˜ KDC ì¶”ì²œ (OpenAI 1.6.0+ ë°©ì‹ìœ¼ë¡œ ë¦¬íŒ©í† ë§)
def recommend_kdc(title, author, api_key):
    try:
        # ğŸ”‘ ë¹„ë°€ì˜ ì—´ì‡ ë¡œ í´ë¼ì´ì–¸íŠ¸ë¥¼ ê¹¨ì›ë‹ˆë‹¤
        client = OpenAI(api_key=api_key)

        # ğŸ“œ ì£¼ë¬¸ë¬¸ì„ ì¤€ë¹„í•˜ê³ 
        prompt = (
            f"ë„ì„œ ì œëª©: {title}\n"
            f"ì €ì: {author}\n"
            "ì´ ì±…ì˜ ì£¼ì œë¥¼ ê³ ë ¤í•˜ì—¬ í•œêµ­ì‹­ì§„ë¶„ë¥˜(KDC) ë²ˆí˜¸ í•˜ë‚˜ë¥¼ ì¶”ì²œí•´ ì£¼ì„¸ìš”.\n"
            "ì •í™•í•œ ìˆ«ìë§Œ ì•„ë˜ í˜•ì‹ìœ¼ë¡œ ê°„ë‹¨íˆ ì‘ë‹µí•´ ì£¼ì„¸ìš”:\n"
            "KDC: 813.7"
        )

        # ğŸ§  GPTì˜ ì§€í˜œë¥¼ ì†Œí™˜
        response = client.chat.completions.create(
            model="gpt-4",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3,
        )

        # â† ì—¬ê¸°ë¶€í„° ë³´ê°•ëœ ë¶€ë¶„
        msg = response.choices[0].message
        content = getattr(msg, "content", None)
        if content is None and isinstance(msg, dict):
            content = msg.get("content", "")
        content = content or ""

        # âœ‚ï¸ â€œKDC:â€ ë’¤ì˜ ìˆ«ìë§Œ êº¼ë‚´ì„œ ëŒë ¤ë“œë¦½ë‹ˆë‹¤
        for line in content.splitlines():
            if "KDC:" in line:
                return line.split("KDC:")[1].strip()

    except Exception as e:
        st.warning(f"ğŸ§  GPT ì˜¤ë¥˜: {e}")

    # ğŸ›¡ï¸ ë§Œì•½ ì‹¤íŒ¨í•˜ë©´ ë””í´íŠ¸ â€œ000â€
    return "000"


# ğŸ“¡ ë¶€ê°€ê¸°í˜¸ ì¶”ì¶œ (êµ­ë¦½ì¤‘ì•™ë„ì„œê´€)
@st.cache_data(ttl=24*3600)
def fetch_additional_code_from_nlk(isbn: str) -> str:
    url = (
        f"https://www.nl.go.kr/seoji/SearchApi.do?"
        f"cert_key={nlk_key}&result_style=xml"
        f"&page_no=1&page_size=1&isbn={isbn}"
    )
    try:
        res = _nlk_session.get(url, timeout=3)  # 3ì´ˆë§Œ ê¸°ë‹¤ë¦¬ê³ 
        res.raise_for_status()
        root = ET.fromstring(res.text)
        doc  = root.find('.//docs/e')
        return (doc.findtext('EA_ADD_CODE') or "").strip() if doc is not None else ""
    except Exception:
        st.warning("âš ï¸ êµ­ì¤‘API ì§€ì—°, ë¶€ê°€ê¸°í˜¸ëŠ” ìƒëµí•©ë‹ˆë‹¤.")
        return ""


# ğŸ”¤ ì–¸ì–´ ê°ì§€ ë° 041, 546 ìƒì„±
ISDS_LANGUAGE_CODES = {
    'kor': 'í•œêµ­ì–´', 'eng': 'ì˜ì–´', 'jpn': 'ì¼ë³¸ì–´', 'chi': 'ì¤‘êµ­ì–´', 'rus': 'ëŸ¬ì‹œì•„ì–´',
    'ara': 'ì•„ëì–´', 'fre': 'í”„ë‘ìŠ¤ì–´', 'ger': 'ë…ì¼ì–´', 'ita': 'ì´íƒˆë¦¬ì•„ì–´', 'spa': 'ìŠ¤í˜ì¸ì–´',
    'und': 'ì•Œ ìˆ˜ ì—†ìŒ'
}

def detect_language(text):
    text = re.sub(r'[\s\W_]+', '', text)
    if not text:
        return 'und'
    first_char = text[0]
    if '\uac00' <= first_char <= '\ud7a3':
        return 'kor'
    elif '\u3040' <= first_char <= '\u30ff':
        return 'jpn'
    elif '\u4e00' <= first_char <= '\u9fff':
        return 'chi'
    elif '\u0400' <= first_char <= '\u04FF':
        return 'rus'
    elif 'a' <= first_char.lower() <= 'z':
        return 'eng'
    else:
        return 'und'

def generate_546_from_041_kormarc(marc_041: str) -> str:
    a_codes, h_code = [], None
    for part in marc_041.split():
        if part.startswith("$a"):
            a_codes.append(part[2:])
        elif part.startswith("$h"):
            h_code = part[2:]
    if len(a_codes) == 1:
        a_lang = ISDS_LANGUAGE_CODES.get(a_codes[0], "ì•Œ ìˆ˜ ì—†ìŒ")
        if h_code:
            h_lang = ISDS_LANGUAGE_CODES.get(h_code, "ì•Œ ìˆ˜ ì—†ìŒ")
            return f"{a_lang}ë¡œ ì”€, ì›ì €ëŠ” {h_lang}ì„"
        else:
            return f"{a_lang}ë¡œ ì”€"
    elif len(a_codes) > 1:
        langs = [ISDS_LANGUAGE_CODES.get(code, "ì•Œ ìˆ˜ ì—†ìŒ") for code in a_codes]
        return f"{'ã€'.join(langs)} ë³‘ê¸°"
    return "ì–¸ì–´ ì •ë³´ ì—†ìŒ"

def crawl_aladin_original_and_price(isbn13):
    url = f"https://www.aladin.co.kr/shop/wproduct.aspx?ISBN={isbn13}"
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        res = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        original = soup.select_one("div.info_original")
        price = soup.select_one("span.price2")
        return {
            "original_title": original.text.strip() if original else "",
            "price": price.text.strip().replace("ì •ê°€ : ", "").replace("ì›", "").replace(",", "").strip() if price else ""
        }
    except:
        return {}

# ---- 653 ì „ì²˜ë¦¬ ìœ í‹¸ ----
def _norm(text: str) -> str:
    import re, unicodedata
    if not text:
        return ""
    text = unicodedata.normalize("NFKC", text).lower()
    text = re.sub(r"[^\w\s\uac00-\ud7a3]", " ", text)  # í•œ/ì˜/ìˆ«ì/ê³µë°±ë§Œ
    return re.sub(r"\s+", " ", text).strip()

def _clean_author_str(s: str) -> str:
    import re
    if not s:
        return ""
    s = re.sub(r"\(.*?\)", " ", s)      # (ì§€ì€ì´), (ì˜®ê¸´ì´) ë“± ì œê±°
    s = re.sub(r"[/;Â·,]", " ", s)       # êµ¬ë¶„ì ê³µë°±í™”
    return re.sub(r"\s+", " ", s).strip()

def _build_forbidden_set(title: str, authors: str) -> set:
    t_norm = _norm(title)
    a_norm = _norm(authors)
    forb = set()
    if t_norm:
        forb.update(t_norm.split())
        forb.add(t_norm.replace(" ", ""))  # 'ì£½ìŒ íŠ¸ë¦´ë¡œì§€' â†’ 'ì£½ìŒíŠ¸ë¦´ë¡œì§€'
    if a_norm:
        forb.update(a_norm.split())
        forb.add(a_norm.replace(" ", ""))
    return {f for f in forb if f and len(f) >= 2}  # 1ê¸€ì ì œê±°

def _should_keep_keyword(kw: str, forbidden: set) -> bool:
    n = _norm(kw)
    if not n or len(n.replace(" ", "")) < 2:
        return False
    for tok in forbidden:
        if tok in n or n in tok:
            return False
    return True
# -------------------------

# ğŸ“„ 653 í•„ë“œ í‚¤ì›Œë“œ ìƒì„±
# â‘¡ ì•Œë¼ë”˜ ë©”íƒ€ë°ì´í„° í˜¸ì¶œ í•¨ìˆ˜
def fetch_aladin_metadata(isbn):
    url = (
        "http://www.aladin.co.kr/ttb/api/ItemLookUp.aspx"
        f"?ttbkey={aladin_key}"
        "&ItemIdType=ISBN"
        f"&ItemId={isbn}"
        "&output=js"
        "&Version=20131101"
        "&OptResult=Toc"
    )
    data = requests.get(url).json()
    item = (data.get("item") or [{}])[0]

    # ì €ì í•„ë“œ ë‹¤ì–‘í•œ í‚¤ ëŒ€ì‘
    raw_author = item.get("author") or item.get("authors") or item.get("author_t") or ""
    authors = _clean_author_str(raw_author)

    return {
        "category": item.get("categoryName", "") or "",
        "title": item.get("title", "") or "",
        "authors": authors,                           # â¬…ï¸ ì¶”ê°€ë¨
        "description": item.get("description", "") or "",
        "toc": item.get("toc", "") or "",
    }



# â‘¢ GPT-4 ê¸°ë°˜ 653 ìƒì„± í•¨ìˆ˜
def generate_653_with_gpt(category, title, authors, description, toc, max_keywords=7):
    parts = [p.strip() for p in (category or "").split(">") if p.strip()]
    cat_kw = parts[-1] if parts else ""

    forbidden = _build_forbidden_set(title, authors)

    system_msg = {
        "role": "system",
        "content": (
            "ë‹¹ì‹ ì€ ë„ì„œê´€ ë©”íƒ€ë°ì´í„° ì „ë¬¸ê°€ì…ë‹ˆë‹¤. "
            "ì±…ì˜ ë¶„ë¥˜, ì„¤ëª…, ëª©ì°¨ë¥¼ ë°”íƒ•ìœ¼ë¡œ MARC 653 ì£¼ì œì–´ë¥¼ ë„ì¶œí•˜ì„¸ìš”. "
            "ì„œëª…(245)Â·ì €ì(100/700)ì— ì¡´ì¬í•˜ëŠ” ë‹¨ì–´ëŠ” ì œì™¸í•©ë‹ˆë‹¤."
        )
    }
    user_msg = {
        "role": "user",
        "content": (
            f"ì…ë ¥ ì •ë³´ë¡œë¶€í„° ìµœëŒ€ {max_keywords}ê°œì˜ MARC 653 ì£¼ì œì–´ë¥¼ í•œ ì¤„ë¡œ ì¶œë ¥í•´ ì£¼ì„¸ìš”.\n\n"
            f"- ë¶„ë¥˜: \"{cat_kw}\"\n"
            f"- ì œëª©(245): \"{title}\"\n"
            f"- ì €ì(100/700): \"{authors}\"\n"
            f"- ì„¤ëª…: \"{description}\"\n"
            f"- ëª©ì°¨: \"{toc}\"\n\n"
            "ì œì™¸ì–´ ëª©ë¡(ì„œëª…/ì €ìì—ì„œ ìœ ë˜): "
            f"{', '.join(sorted(forbidden)) or '(ì—†ìŒ)'}\n\n"
            "ê·œì¹™:\n"
            "1) 'ì œëª©'ê³¼ 'ì €ì'ì— ì“°ì¸ ë‹¨ì–´Â·í‘œí˜„ì€ ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.\n"
            "2) ë¶„ë¥˜/ì„¤ëª…/ëª©ì°¨ì—ì„œ í•µì‹¬ ê°œë…ì„ ëª…ì‚¬ ì¤‘ì‹¬ìœ¼ë¡œ ë½‘ìœ¼ì„¸ìš”.\n"
            "3) ì¶œë ¥ í˜•ì‹: $aí‚¤ì›Œë“œ1 $aí‚¤ì›Œë“œ2 â€¦ (í•œ ì¤„)\n"
        )
    }
    try:
        resp = gpt_client.chat.completions.create(
            model="gpt-4",
            messages=[system_msg, user_msg],
            temperature=0.2,
            max_tokens=180,
        )
        raw = (resp.choices[0].message.content or "").strip()

        # $a ë‹¨ìœ„ íŒŒì‹±
        pattern = re.compile(r"\$a(.*?)(?=(?:\$a|$))", re.DOTALL)
        kws = [m.group(1).strip() for m in pattern.finditer(raw)]
        if not kws:
            # ë°±ì—… íŒŒì‹±
            tmp = re.split(r"[,\n]", raw)
            kws = [t.strip().lstrip("$a") for t in tmp if t.strip()]

        # ê³µë°± ì‚­ì œ(ì›í•˜ë©´ ìœ ì§€ ê°€ëŠ¥)
        kws = [kw.replace(" ", "") for kw in kws]

        # 1ì°¨: ê¸ˆì¹™ì–´(ì„œëª…/ì €ì) í•„í„°
        kws = [kw for kw in kws if _should_keep_keyword(kw, forbidden)]

        # 2ì°¨: ì •ê·œí™” ì¤‘ë³µ ì œê±°
        seen = set()
        uniq = []
        for kw in kws:
            n = _norm(kw)
            if n not in seen:
                seen.add(n)
                uniq.append(kw)

        # 3ì°¨: ìµœëŒ€ ê°œìˆ˜ ì œí•œ
        uniq = uniq[:max_keywords]

        return "".join(f"$a{kw}" for kw in uniq)

    except Exception as e:
        st.warning(f"âš ï¸ 653 ì£¼ì œì–´ ìƒì„± ì‹¤íŒ¨: {e}")
        return None
   


# ğŸ“š MARC ìƒì„±
@st.cache_data(show_spinner=False)
def fetch_book_data_from_aladin(isbn, reg_mark="", reg_no="", copy_symbol=""):
    import re
    from concurrent.futures import ThreadPoolExecutor

    # 1) ì•Œë¼ë”˜ + (ì˜µì…˜) êµ­ì¤‘ ë¶€ê°€ê¸°í˜¸ ë™ì‹œ ìš”ì²­
    url = (
        f"https://www.aladin.co.kr/ttb/api/ItemLookUp.aspx?"
        f"ttbkey={aladin_key}&itemIdType=ISBN&ItemId={isbn}"
        f"&output=js&Version=20131101"
    )
    with ThreadPoolExecutor(max_workers=2) as ex:
        future_aladin = ex.submit(lambda: requests.get(url, verify=False, timeout=5))
        future_nlk    = ex.submit(fetch_additional_code_from_nlk, isbn)

        try:
            resp = future_aladin.result()
            resp.raise_for_status()
            data = resp.json().get("item", [{}])[0]
        except Exception as e:
            st.error(f"ğŸš¨ ì•Œë¼ë”˜API ì˜¤ë¥˜: {e}")
            return ""

        add_code = future_nlk.result()  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´

    # 2) ë©”íƒ€ë°ì´í„° (ì•Œë¼ë”˜)
    title       = data.get("title",       "ì œëª©ì—†ìŒ")
    author      = data.get("author",      "ì €ìë¯¸ìƒ")
    publisher   = data.get("publisher",   "ì¶œíŒì‚¬ë¯¸ìƒ")
    pubdate     = data.get("pubDate",     "2025")  # 'YYYY' ë˜ëŠ” 'YYYY-MM-DD'
    category    = data.get("categoryName", "")
    description = data.get("description", "")
    toc         = data.get("subInfo", {}).get("toc", "")
    price       = str(data.get("priceStandard", ""))  # 020/950 ìš©

    # 3) =008 ìƒì„± (ISBNë§Œìœ¼ë¡œ ìë™, country/langì€ ì„ì‹œ ê³ ì •ê°’ â†’ ì¶”í›„ override)
    tag_008 = "=008  " + build_008_from_isbn(
        isbn,
        aladin_pubdate=pubdate,
        aladin_title=title,
        aladin_category=category,
        aladin_desc=description,
        # override_country3="ulk",  # 300 ëª¨ë“ˆ ì™„ì„± ì‹œ ì‚¬ìš©
        # override_lang3="kor",     # 041 ëª¨ë“ˆ ì™„ì„± ì‹œ ì‚¬ìš©
    )

    # 4) 041/546 (ê°„ì´ ê°ì§€: ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    lang_a  = detect_language(title)
    lang_h  = detect_language(data.get("title", ""))
    tag_041 = f"=041  \\$a{lang_a}" + (f"$h{lang_h}" if lang_h != "und" else "")
    tag_546 = f"=546  \\$a{generate_546_from_041_kormarc(tag_041)}"

    # 5) 020 (ë¶€ê°€ê¸°í˜¸ ìˆìœ¼ë©´ $g ì¶”ê°€)
    tag_020 = f"=020  \\$a{isbn}"
    if price:
        tag_020 += f":$c{price}"
    if add_code:
        tag_020 += f"$g{add_code}"


    # 6) 653/KDC â€” âœ… ì—¬ê¸°ì„œë§Œ ìƒì„± (GPTAPI ìµœì‹  í•¨ìˆ˜ë¡œ í†µì¼)
    kdc     = recommend_kdc(title, author, api_key=openai_key)

    # â¬‡ï¸ authors ì¸ì ì¶”ê°€(ì €ì ë¬¸ìì—´ì„ ì „ì²˜ë¦¬í•´ì„œ ë„˜ê¹€)
    gpt_653 = generate_653_with_gpt(
    category,
    title,
    _clean_author_str(author),   # â† ì¶”ê°€ëœ ë¶€ë¶„
    description,
    toc,
    max_keywords=7
    )

    tag_653 = f"=653  \\{gpt_653.replace(' ', '')}" if gpt_653 else ""


    # 7) ê¸°ë³¸ MARC ë¼ì¸
    marc_lines = [
        tag_008,
        "=007  ta",
        f"=245  00$a{title} /$c{author}",
        f"=260  \\$aì„œìš¸ :$b{publisher},$c{pubdate[:4]}.",
    ]

    # 8) 490Â·830 (ì´ì„œ)
    series = data.get("seriesInfo", {})
    name = (series.get("seriesName") or "").strip()
    vol  = (series.get("volume")    or "").strip()
    if name:
        marc_lines.append(f"=490  \\$a{name};$v{vol}")
        marc_lines.append(f"=830  \\$a{name};$v{vol}")

    # 9) ê¸°íƒ€ í•„ë“œ
    marc_lines.append(tag_020)
    marc_lines.append(tag_041)
    marc_lines.append(tag_546)
    if kdc and kdc != "000":
        marc_lines.append(f"=056  \\$a{kdc}$26")
    if tag_653:
        marc_lines.append(tag_653)
    marc_lines.append(f"=950  0\\$b{price}")

    # 10) 049: ì†Œì¥ê¸°í˜¸(ì…ë ¥ëœ ê²½ìš°ë§Œ)
    if reg_mark or reg_no or copy_symbol:
        line = f"=049  0\\$I{reg_mark}{reg_no}"
        if copy_symbol:
            line += f"$f{copy_symbol}"
        marc_lines.append(line)

    # 11) ë²ˆí˜¸ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ í›„ ì¶œë ¥
    marc_lines.sort(key=lambda L: int(re.match(r"=(\d+)", L).group(1)))
    return "\n".join(marc_lines)




# ğŸ›ï¸ Streamlit UI
st.title("ğŸ“š ISBN to MARC ë³€í™˜ê¸° (í†µí•©ë²„ì „)")

isbn_list = []
single_isbn = st.text_input("ğŸ”¹ ë‹¨ì¼ ISBN ì…ë ¥", placeholder="ì˜ˆ: 9788936434267")
if single_isbn.strip():
    isbn_list = [[single_isbn.strip(), "", "", ""]]

uploaded_file = st.file_uploader("ğŸ“ CSV ì—…ë¡œë“œ (ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸)", type="csv")
if uploaded_file:
    df = pd.read_csv(uploaded_file)
    if {'ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸'}.issubset(df.columns):
        isbn_list = df[['ISBN', 'ë“±ë¡ê¸°í˜¸', 'ë“±ë¡ë²ˆí˜¸', 'ë³„ì¹˜ê¸°í˜¸']].dropna(subset=['ISBN']).values.tolist()
    else:
        st.error("âŒ í•„ìš”í•œ ì—´ì´ ì—†ìŠµë‹ˆë‹¤: ISBN, ë“±ë¡ê¸°í˜¸, ë“±ë¡ë²ˆí˜¸, ë³„ì¹˜ê¸°í˜¸")

if isbn_list:
    st.subheader("ğŸ“„ MARC ì¶œë ¥")
    marc_results = []
    for row in isbn_list:
        isbn, reg_mark, reg_no, copy_symbol = row
        marc = fetch_book_data_from_aladin(isbn, reg_mark, reg_no, copy_symbol)
        if marc:
            st.code(marc, language="text")
            marc_results.append(marc)

    full_text = "\n\n".join(marc_results)
    st.download_button("ğŸ“¦ ëª¨ë“  MARC ë‹¤ìš´ë¡œë“œ", data=full_text, file_name="marc_output.txt", mime="text/plain")

# ğŸ“„ í…œí”Œë¦¿ ì˜ˆì‹œ ë‹¤ìš´ë¡œë“œ
example_csv = "ISBN,ë“±ë¡ê¸°í˜¸,ë“±ë¡ë²ˆí˜¸,ë³„ì¹˜ê¸°í˜¸\n9791173473968,JUT,12345,TCH\n"
buffer = io.BytesIO()
buffer.write(example_csv.encode("utf-8-sig"))
buffer.seek(0)
st.download_button("ğŸ“„ ì„œì‹ íŒŒì¼ ë‹¤ìš´ë¡œë“œ", data=buffer, file_name="isbn_template.csv", mime="text/csv")

# â¬‡ï¸ í•˜ë‹¨ ë§ˆí¬
st.markdown("""
<div style='text-align: center; font-size: 14px; color: gray;'>
ğŸ“š <strong>ë„ì„œ DB ì œê³µ</strong> : <a href='https://www.aladin.co.kr' target='_blank'>ì•Œë¼ë”˜ ì¸í„°ë„·ì„œì (www.aladin.co.kr)</a>
</div>
""", unsafe_allow_html=True)
